{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "- é¦–å…ˆéœ€è¦åŠ è½½æ•°æ®é›†ï¼ˆä»æœ¬åœ°åŠ è½½ä¸ä» huggingface åŠ è½½çš„æ–¹å¼ä¸ä¸€æ ·!ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æŸ¥ Mac mps æ˜¯å¦å¯ç”¨ï½ï½ï½\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mps',\n",
       " PNDMScheduler {\n",
       "   \"_class_name\": \"PNDMScheduler\",\n",
       "   \"_diffusers_version\": \"0.12.1\",\n",
       "   \"beta_end\": 0.012,\n",
       "   \"beta_schedule\": \"scaled_linear\",\n",
       "   \"beta_start\": 0.00085,\n",
       "   \"clip_sample\": false,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"set_alpha_to_one\": false,\n",
       "   \"skip_prk_steps\": true,\n",
       "   \"steps_offset\": 1,\n",
       "   \"trained_betas\": null\n",
       " },\n",
       " CLIPTokenizer(name_or_path='model/diffsion_from_scratch.params/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " })"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ MPSï¼ˆApple çš„ GPU åŠ é€Ÿï¼‰\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "print(\"æ£€æŸ¥ Mac mps æ˜¯å¦å¯ç”¨ï½ï½ï½\")\n",
    "print(torch.backends.mps.is_available())  # æ£€æŸ¥ MPS æ˜¯å¦å¯ç”¨\n",
    "print(torch.backends.mps.is_built())      # æ£€æŸ¥ PyTorch æ˜¯å¦æ„å»ºäº† MPS æ”¯æŒ\n",
    "\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ DiffusionPipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'model/diffsion_from_scratch.params', safety_checker=None\n",
    ")\n",
    "\n",
    "\n",
    "# ã€å·¥å…·ç±»ã€‘è·å–è°ƒåº¦å™¨å’Œåˆ†è¯å™¨\n",
    "scheduler = pipeline.scheduler # scheduler æ˜¯å¾€å›¾ç‰‡ä¸­æ·»åŠ å™ªå£°çš„æ–¹æ³•\n",
    "tokenizer = pipeline.tokenizer\n",
    "\n",
    "# é‡Šæ”¾ pipeline èµ„æº\n",
    "del pipeline\n",
    "\n",
    "# æ‰“å°è®¾å¤‡ã€è°ƒåº¦å™¨å’Œåˆ†è¯å™¨\n",
    "device, scheduler, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  \\\n",
      "0  {'bytes': b'\\xff\\xd8\\xff\\xe1#\\rExif\\x00\\x00MM\\...   \n",
      "1  {'bytes': b'\\xff\\xd8\\xff\\xe1#\\x98Exif\\x00\\x00M...   \n",
      "2  {'bytes': b'\\xff\\xd8\\xff\\xe1\\x18\\x82Exif\\x00\\x...   \n",
      "3  {'bytes': b'\\xff\\xd8\\xff\\xe1\\x1a\\x06Exif\\x00\\x...   \n",
      "4  {'bytes': b'\\xff\\xd8\\xff\\xe1\\x1d\\xe3Exif\\x00\\x...   \n",
      "\n",
      "                                                text  \n",
      "0         a drawing of a green pokemon with red eyes  \n",
      "1             a green and yellow toy with a red nose  \n",
      "2  a red and white ball with an angry look on its...  \n",
      "3           a cartoon ball with a smile on it's face  \n",
      "4          a bunch of balls with faces drawn on them  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 833 entries, 0 to 832\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   image   833 non-null    object\n",
      " 1   text    833 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.1+ KB\n",
      "ğŸ‘€ None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['pixel_values', 'input_ids'],\n",
       "     num_rows: 833\n",
       " }),\n",
       " {'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "  'input_ids': tensor([49406,   320,  3610,   539,   320,  1901,  9528,   593,   736,  3095,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407])})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision\n",
    "import pandas as pd # ç”¨äºæŸ¥çœ‹æ•°æ®é›†\n",
    "from PIL import Image\n",
    "import io  # ç¡®ä¿å¯¼å…¥ io æ¨¡å—\n",
    "\n",
    "\n",
    "\n",
    "# åŠ è½½å®å¯æ¢¦ parquet æ•°æ®é›†\n",
    "# dataset = load_dataset(path='Datasets/lansinuotediffsion_from_scratch', split='train')\n",
    "data_path = 'Datasets/lansinuotediffsion_from_scratch/train-00000-of-00001-4f5339e7acda17d8.parquet'\n",
    "dataset = load_dataset('parquet', data_files=data_path, split='train')\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# æŸ¥çœ‹å‰å‡ è¡Œæ•°æ®\n",
    "print(df.head())\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯\n",
    "print(\"ğŸ‘€\", df.info())\n",
    "\n",
    "\n",
    "# å›¾åƒå¢å¼ºæ¨¡å—\n",
    "compose = torchvision.transforms.Compose([\n",
    "    # å°†å›¾åƒè°ƒæ•´ä¸º 512x512 åƒç´ ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼\n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    # ä»å›¾åƒä¸­å¿ƒè£å‰ªå‡º 512x512 åƒç´ çš„éƒ¨åˆ†\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    # torchvison.transforms.RandomHorizontalFlip(),  # å¯é€‰ï¼šéšæœºæ°´å¹³ç¿»è½¬å›¾åƒ\n",
    "    # å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡æ ¼å¼\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # å¯¹å¼ é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå‡å€¼å’Œæ ‡å‡†å·®å‡ä¸º 0.5\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# # åˆ†åˆ«å¯¹æ•°æ®é›†ä¸­çš„å›¾ç‰‡å’Œæ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "# def f(data):\n",
    "#     # åº”ç”¨å›¾åƒå¢å¼º\n",
    "#     pixel_values = [compose(i) for i in data['image']]\n",
    "\n",
    "#     # æ–‡å­—ç¼–ç \n",
    "#     input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "#                                             padding='max_length',\n",
    "#                                             truncation=True,\n",
    "#                                             max_length=77).input_ids\n",
    "\n",
    "#     return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "# åˆ†åˆ«å¯¹æ•°æ®é›†ä¸­çš„å›¾ç‰‡å’Œæ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "def f(data):\n",
    "    # ğŸ”¥ğŸ”¥ ä»å­—å…¸ä¸­æå–å­—èŠ‚æ•°æ®å¹¶è½¬æ¢ä¸ºå›¾åƒå¯¹è±¡ => ä»æœ¬åœ° dataset åŠ è½½æ‰éœ€è¦\n",
    "    images = [Image.open(io.BytesIO(image_data['bytes'])) for image_data in data['image']]\n",
    "    pixel_values = [compose(image) for image in images]\n",
    "\n",
    "    # æ–‡å­—ç¼–ç \n",
    "    input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "                                            padding='max_length',\n",
    "                                            truncation=True,\n",
    "                                            max_length=77).input_ids\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "# å¯¹æ•°æ®é›†åº”ç”¨ç¼–ç å‡½æ•° f\n",
    "dataset = dataset.map(f,\n",
    "                      batched=True,   # å¯ç”¨æ‰¹å¤„ç†\n",
    "                      batch_size=100, # æ¯æ¬¡å¤„ç† 100 ä¸ªæ ·æœ¬\n",
    "                      num_proc=1,     # ä½¿ç”¨ 1 ä¸ªè¿›ç¨‹\n",
    "                      remove_columns=['image', 'text'])  # ç§»é™¤åŸå§‹çš„å›¾åƒå’Œæ–‡æœ¬åˆ—\n",
    "\n",
    "# è®¾ç½®æ•°æ®é›†æ ¼å¼ä¸º PyTorch\n",
    "dataset.set_format(type='torch')\n",
    "\n",
    "# è¿”å›å¤„ç†åçš„æ•°æ®é›†å’Œç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "dataset, dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833,\n",
       " {'pixel_values': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]]]], device='mps:0'),\n",
       "  'input_ids': tensor([[49406,   320,  7651,  6575,   593,   902,  4932,  3184,  1488, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='mps:0')})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å®šä¹‰loader - æ‰¹é‡åŠ è½½æ•°æ®\n",
    "# å°†æ•°æ®é›†åˆ†æˆæ›´å°çš„æ‰¹æ¬¡ï¼ˆbatchï¼‰ï¼Œæ¯æ¬¡å¤„ç†ä¸€æ‰¹æ•°æ®ã€‚è¿™æ ·ï¼Œæ¨¡å‹åœ¨æ¯æ¬¡è¿­ä»£æ—¶åªéœ€å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œå‡å°‘å†…å­˜å ç”¨ï¼Œå¹¶ä¸”å¯ä»¥æ›´å¿«åœ°è¿›è¡Œè®­ç»ƒ\n",
    "def collate_fn(data):\n",
    "    pixel_values = [i['pixel_values'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values).to(device)\n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     batch_size=1)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'diffsion_from_scratch.params', 'diffsion_from_scratch.unet']\n",
      "âœ… Model loaded successfully!\n",
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "ğŸ‘€ ä¸¤ä¸ªæ¨¡å‹çš„å¼ é‡æ˜¯å¦ç›¸è¿‘ True\n",
      "tensor(1.8835e-05, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_attention_op', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_use_memory_efficient_attention_xformers', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'channels', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'group_norm', 'half', 'ipu', 'key', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_head_size', 'num_heads', 'parameters', 'proj_attn', 'query', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'rescale_output_factor', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'set_use_memory_efficient_attention_xformers', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'value', 'xpu', 'zero_grad']\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_attention_op', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_use_memory_efficient_attention_xformers', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'channels', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'group_norm', 'half', 'ipu', 'key', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_head_size', 'num_heads', 'parameters', 'proj_attn', 'query', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'rescale_output_factor', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'set_use_memory_efficient_attention_xformers', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'value', 'xpu', 'zero_grad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 1e-05\n",
       "     maximize: False\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " MSELoss())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jupiter åŠ è½½æ¨¡å‹çš„å‘½ä»¤, åŠ è½½ä¹‹å‰ç”Ÿæˆçš„å‡ ä¸ªæ¨¡å‹\n",
    "%run 1.encoder.ipynb\n",
    "%run 2.vae.ipynb\n",
    "%run 3.unet.ipynb\n",
    "\n",
    "# å‡†å¤‡è®­ç»ƒ (åªè®­ç»ƒ U-Net, å›ºå®š VAE å’Œ Encoder, å› æ­¤ä¸‹é¢å°±å†»ç»“å…¶ä»–ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°)\n",
    "# å†»ç»“ç¼–ç å™¨æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒæ—¶ä¸æ›´æ–°\n",
    "encoder.requires_grad_(False)\n",
    "# å†»ç»“ VAE æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒæ—¶ä¸æ›´æ–°\n",
    "vae.requires_grad_(False)\n",
    "# ä½¿ U-Net æ¨¡å‹çš„å‚æ•°å¯è®­ç»ƒï¼Œå³åœ¨è®­ç»ƒæ—¶æ›´æ–°\n",
    "unet.requires_grad_(True)\n",
    "\n",
    "# è®¾ç½®ç¼–ç å™¨å’Œ VAE ä¸ºè¯„ä¼°æ¨¡å¼ (å³å†»ç»“å®ƒä»¬çš„è¡Œä¸ºï¼Œé˜²æ­¢ BatchNorm å’Œ Dropout ç­‰æ“ä½œå½±å“)\n",
    "encoder.eval()\n",
    "vae.eval()\n",
    "# è®¾ç½® U-Net ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ BatchNorm å’Œ Dropout ç­‰è®­ç»ƒæ—¶ç‰¹æœ‰çš„æ“ä½œï¼‰\n",
    "unet.train()\n",
    "\n",
    "# å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰\n",
    "encoder.to(device)\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "# è®¾ç½®ä¼˜åŒ–å™¨ï¼Œåªä¼˜åŒ– U-Net çš„å‚æ•°\n",
    "# ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º 1e-5ï¼Œbeta1 å’Œ beta2 æ˜¯åŠ¨é‡é¡¹çš„è¡°å‡ç‡ï¼Œweight_decay æ˜¯æƒé‡è¡°å‡ç³»æ•°ï¼Œeps æ˜¯ä¸ºæ•°å€¼ç¨³å®šæ€§æ·»åŠ çš„å°æ•°å€¼\n",
    "optimizer = torch.optim.AdamW(unet.parameters(),\n",
    "                              lr=1e-5,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "# ä½¿ç”¨å‡æ–¹è¯¯å·® (MSE) ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡ä¹‹é—´çš„å·®å¼‚\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# è¾“å‡ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°çš„é…ç½®\n",
    "optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0096, device='mps:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¦‚æœæ¨¡å‹é¢„æµ‹çš„ç»“æœå’ŒçœŸå®ç­”æ¡ˆéå¸¸æ¥è¿‘ï¼Œloss å°±ä¼šå¾ˆå°ï¼›å¦‚æœé¢„æµ‹ç»“æœå’ŒçœŸå®ç­”æ¡ˆç›¸å·®å¾ˆè¿œï¼Œloss å°±ä¼šå¾ˆå¤§\n",
    "def get_loss(data): # å…¥å‚æ˜¯ä¸€æ‰¹ã€å›¾ç‰‡æ•°æ®ã€‘å’Œã€æ–‡æœ¬æ•°æ®ã€‘\n",
    "    with torch.no_grad():\n",
    "        #æ–‡å­—ç¼–ç \n",
    "        #[1, 77] -> [1, 77, 768]\n",
    "        out_encoder = encoder(data['input_ids'])\n",
    "\n",
    "        #æŠ½å–å›¾åƒç‰¹å¾å›¾\n",
    "        #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "        out_vae = vae.encoder(data['pixel_values'])\n",
    "        out_vae = vae.sample(out_vae)\n",
    "\n",
    "        #0.18215 = vae.config.scaling_factor\n",
    "        out_vae = out_vae * 0.18215\n",
    "\n",
    "    # éšæœºæ•°,unetçš„è®¡ç®—ç›®æ ‡\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "\n",
    "    # å¾€ç‰¹å¾å›¾ä¸­æ·»åŠ å™ªå£°\n",
    "    #1000 = scheduler.num_train_timesteps\n",
    "    #1 = batch size\n",
    "    # æ·»åŠ éšæœºå™ªå£°\n",
    "    noise_step = torch.randint(0, 1000, (1, )).long().to(device)\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    # æ ¹æ®æ–‡å­—ä¿¡æ¯,æŠŠç‰¹å¾å›¾ä¸­çš„å™ªå£°è®¡ç®—å‡ºæ¥\n",
    "    out_unet = unet(out_vae=out_vae_noise,\n",
    "                    out_encoder=out_encoder,\n",
    "                    time=noise_step)\n",
    "\n",
    "    # è®¡ç®— mse loss\n",
    "    #[1, 4, 64, 64],[1, 4, 64, 64]\n",
    "    return criterion(out_unet, noise)\n",
    "\n",
    "\n",
    "get_loss({\n",
    "    'input_ids': torch.ones(1, 77, device=device).long(),\n",
    "    'pixel_values': torch.randn(1, 3, 512, 512, device=device)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.482576073292876\n",
      "10 103.78535157235456\n",
      "20 101.80729839394917\n",
      "30 97.18179716335726\n",
      "40 95.27005371931591\n",
      "50 93.52235668250069\n",
      "60 91.23645739001222\n",
      "70 85.15300659097556\n",
      "80 86.9126384588817\n",
      "90 81.5698299240612\n",
      "100 78.39926671008288\n",
      "110 75.35619182675873\n",
      "120 74.10794699968392\n",
      "130 71.74995687346382\n",
      "140 68.43087464362179\n",
      "150 63.701912391486985\n",
      "160 64.13840378901659\n",
      "170 59.5763320050537\n",
      "180 58.09928199512069\n",
      "190 54.098192290555744\n",
      "200 53.715583391978726\n",
      "210 49.144029857572605\n",
      "220 47.92628015137598\n",
      "230 45.43078308462282\n",
      "240 42.47678213690597\n",
      "250 42.57540527023593\n",
      "260 38.39479267646675\n",
      "270 36.950618647257215\n",
      "280 37.183194903125695\n",
      "290 33.844317765288\n",
      "300 32.76033932725841\n",
      "310 30.12229502759874\n",
      "320 30.798989617975167\n",
      "330 30.148435787028575\n",
      "340 27.67200335358939\n",
      "350 27.441537938197143\n",
      "360 26.668230306680925\n",
      "370 26.568718082347914\n",
      "380 24.796474545077217\n",
      "390 24.26365733845887\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory saves does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(unet\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaves/unet.model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         loss_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaves/unet.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/torch/serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory saves does not exist."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ğŸ‘‡ğŸ‘‡ å¼€å§‹è®­ç»ƒ\n",
    "def train():\n",
    "    loss_sum = 0\n",
    "    # ğŸš€ğŸš€ è®­ç»ƒ 400 ä¸ª epoch\n",
    "    for epoch in range(400):\n",
    "        # éå†æ¯ä¸€ä¸ª batch çš„æ•°æ®\n",
    "        for i, data in enumerate(loader):\n",
    "            # è®¡ç®—æŸå¤±å¹¶åšåå‘ä¼ æ’­ï¼ˆloss.backwardï¼‰ï¼Œä½†ä¸ç«‹å³æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "            loss = get_loss(data) / 4  # æ¯ 4 ä¸ªæ‰¹æ¬¡åšä¸€æ¬¡å‚æ•°çš„è°ƒæ•´\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            # æ¯ 4 ä¸ªæ‰¹æ¬¡æ›´æ–°ä¸€æ¬¡æ¨¡å‹å‚æ•°\n",
    "            if (epoch * len(loader) + i) % 4 == 0:\n",
    "                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "                torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "                optimizer.step()\n",
    "                # æ¸…é›¶ä¼˜åŒ–å™¨çš„æ¢¯åº¦\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # æ¯ 10 ä¸ª epoch è¾“å‡ºä¸€æ¬¡ç´¯è®¡çš„æŸå¤±å€¼å¹¶é‡ç½®\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    # è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„\n",
    "    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º\n",
    "    os.makedirs('saves', exist_ok=True)\n",
    "    torch.save(unet.to('cpu'), 'saves/unet.model')\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "# åŒ…è£…ç±»\n",
    "class Model(PreTrainedModel):\n",
    "    # å®šä¹‰é…ç½®ç±»ï¼Œè¿™é‡Œä½¿ç”¨é¢„è®­ç»ƒé…ç½®\n",
    "    config_class = PretrainedConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # åˆå§‹åŒ–çˆ¶ç±»çš„æ„é€ å‡½æ•°\n",
    "        super().__init__(config)\n",
    "        # å°† U-Net æ¨¡å‹åŠ è½½åˆ° CPU ä¸Šå¹¶ä½œä¸ºç±»çš„ä¸€ä¸ªå±æ€§\n",
    "        self.unet = unet.to('cpu')\n",
    "\n",
    "# ğŸ‘‰ ä¿å­˜æ¨¡å‹åˆ° Hugging Face Hub\n",
    "Model(PretrainedConfig()).push_to_hub(\n",
    "    # æŒ‡å®šè¦ä¸Šä¼ åˆ° Hugging Face Hub çš„æ¨¡å‹åº“ ID\n",
    "    repo_id='lansinuote/diffsion_from_scratch.unet',\n",
    "    # ä½¿ç”¨ä»æ–‡ä»¶ä¸­è¯»å–çš„è®¤è¯ä»¤ç‰Œè¿›è¡Œæˆæƒ\n",
    "    use_auth_token=open('/root/hub_token.txt').read().strip()\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
