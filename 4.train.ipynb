{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练\n",
    "- 首先需要加载数据集（从本地加载与从 huggingface 加载的方式不一样!）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查 Mac mps 是否可用～～～\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mps',\n",
       " PNDMScheduler {\n",
       "   \"_class_name\": \"PNDMScheduler\",\n",
       "   \"_diffusers_version\": \"0.12.1\",\n",
       "   \"beta_end\": 0.012,\n",
       "   \"beta_schedule\": \"scaled_linear\",\n",
       "   \"beta_start\": 0.00085,\n",
       "   \"clip_sample\": false,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"set_alpha_to_one\": false,\n",
       "   \"skip_prk_steps\": true,\n",
       "   \"steps_offset\": 1,\n",
       "   \"trained_betas\": null\n",
       " },\n",
       " CLIPTokenizer(name_or_path='model/diffsion_from_scratch.params/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " })"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# 检查是否可以使用 MPS（Apple 的 GPU 加速）\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "print(\"检查 Mac mps 是否可用～～～\")\n",
    "print(torch.backends.mps.is_available())  # 检查 MPS 是否可用\n",
    "print(torch.backends.mps.is_built())      # 检查 PyTorch 是否构建了 MPS 支持\n",
    "\n",
    "\n",
    "# 从预训练模型加载 DiffusionPipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'model/diffsion_from_scratch.params', safety_checker=None\n",
    ")\n",
    "\n",
    "\n",
    "# 【工具类】获取调度器和分词器\n",
    "scheduler = pipeline.scheduler # scheduler 是往图片中添加噪声的方法\n",
    "tokenizer = pipeline.tokenizer\n",
    "\n",
    "# 释放 pipeline 资源\n",
    "del pipeline\n",
    "\n",
    "# 打印设备、调度器和分词器\n",
    "device, scheduler, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  \\\n",
      "0  {'bytes': b'\\xff\\xd8\\xff\\xe1#\\rExif\\x00\\x00MM\\...   \n",
      "1  {'bytes': b'\\xff\\xd8\\xff\\xe1#\\x98Exif\\x00\\x00M...   \n",
      "2  {'bytes': b'\\xff\\xd8\\xff\\xe1\\x18\\x82Exif\\x00\\x...   \n",
      "3  {'bytes': b'\\xff\\xd8\\xff\\xe1\\x1a\\x06Exif\\x00\\x...   \n",
      "4  {'bytes': b'\\xff\\xd8\\xff\\xe1\\x1d\\xe3Exif\\x00\\x...   \n",
      "\n",
      "                                                text  \n",
      "0         a drawing of a green pokemon with red eyes  \n",
      "1             a green and yellow toy with a red nose  \n",
      "2  a red and white ball with an angry look on its...  \n",
      "3           a cartoon ball with a smile on it's face  \n",
      "4          a bunch of balls with faces drawn on them  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 833 entries, 0 to 832\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   image   833 non-null    object\n",
      " 1   text    833 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.1+ KB\n",
      "👀 None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['pixel_values', 'input_ids'],\n",
       "     num_rows: 833\n",
       " }),\n",
       " {'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "  'input_ids': tensor([49406,   320,  3610,   539,   320,  1901,  9528,   593,   736,  3095,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407])})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision\n",
    "import pandas as pd # 用于查看数据集\n",
    "from PIL import Image\n",
    "import io  # 确保导入 io 模块\n",
    "\n",
    "\n",
    "\n",
    "# 加载宝可梦 parquet 数据集\n",
    "# dataset = load_dataset(path='Datasets/lansinuotediffsion_from_scratch', split='train')\n",
    "data_path = 'Datasets/lansinuotediffsion_from_scratch/train-00000-of-00001-4f5339e7acda17d8.parquet'\n",
    "dataset = load_dataset('parquet', data_files=data_path, split='train')\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# 查看前几行数据\n",
    "print(df.head())\n",
    "\n",
    "# 查看数据集的基本信息\n",
    "print(\"👀\", df.info())\n",
    "\n",
    "\n",
    "# 图像增强模块\n",
    "compose = torchvision.transforms.Compose([\n",
    "    # 将图像调整为 512x512 像素，使用双线性插值\n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    # 从图像中心裁剪出 512x512 像素的部分\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    # torchvison.transforms.RandomHorizontalFlip(),  # 可选：随机水平翻转图像\n",
    "    # 将图像转换为张量格式\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # 对张量进行标准化，均值和标准差均为 0.5\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# # 分别对数据集中的图片和文本进行编码\n",
    "# def f(data):\n",
    "#     # 应用图像增强\n",
    "#     pixel_values = [compose(i) for i in data['image']]\n",
    "\n",
    "#     # 文字编码\n",
    "#     input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "#                                             padding='max_length',\n",
    "#                                             truncation=True,\n",
    "#                                             max_length=77).input_ids\n",
    "\n",
    "#     return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "# 分别对数据集中的图片和文本进行编码\n",
    "def f(data):\n",
    "    # 🔥🔥 从字典中提取字节数据并转换为图像对象 => 从本地 dataset 加载才需要\n",
    "    images = [Image.open(io.BytesIO(image_data['bytes'])) for image_data in data['image']]\n",
    "    pixel_values = [compose(image) for image in images]\n",
    "\n",
    "    # 文字编码\n",
    "    input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "                                            padding='max_length',\n",
    "                                            truncation=True,\n",
    "                                            max_length=77).input_ids\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "# 对数据集应用编码函数 f\n",
    "dataset = dataset.map(f,\n",
    "                      batched=True,   # 启用批处理\n",
    "                      batch_size=100, # 每次处理 100 个样本\n",
    "                      num_proc=1,     # 使用 1 个进程\n",
    "                      remove_columns=['image', 'text'])  # 移除原始的图像和文本列\n",
    "\n",
    "# 设置数据集格式为 PyTorch\n",
    "dataset.set_format(type='torch')\n",
    "\n",
    "# 返回处理后的数据集和第一个样本\n",
    "dataset, dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833,\n",
       " {'pixel_values': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]]]], device='mps:0'),\n",
       "  'input_ids': tensor([[49406,   320,  7651,  6575,   593,   902,  4932,  3184,  1488, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407]], device='mps:0')})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义loader - 批量加载数据\n",
    "# 将数据集分成更小的批次（batch），每次处理一批数据。这样，模型在每次迭代时只需处理一部分数据，减少内存占用，并且可以更快地进行训练\n",
    "def collate_fn(data):\n",
    "    pixel_values = [i['pixel_values'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values).to(device)\n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     batch_size=1)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'diffsion_from_scratch.params', 'diffsion_from_scratch.unet']\n",
      "✅ Model loaded successfully!\n",
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "👀 两个模型的张量是否相近 True\n",
      "tensor(1.8835e-05, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_attention_op', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_use_memory_efficient_attention_xformers', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'channels', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'group_norm', 'half', 'ipu', 'key', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_head_size', 'num_heads', 'parameters', 'proj_attn', 'query', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'rescale_output_factor', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'set_use_memory_efficient_attention_xformers', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'value', 'xpu', 'zero_grad']\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_attention_op', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_use_memory_efficient_attention_xformers', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'channels', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'group_norm', 'half', 'ipu', 'key', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_head_size', 'num_heads', 'parameters', 'proj_attn', 'query', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'rescale_output_factor', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'set_use_memory_efficient_attention_xformers', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'value', 'xpu', 'zero_grad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 1e-05\n",
       "     maximize: False\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " MSELoss())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jupiter 加载模型的命令, 加载之前生成的几个模型\n",
    "%run 1.encoder.ipynb\n",
    "%run 2.vae.ipynb\n",
    "%run 3.unet.ipynb\n",
    "\n",
    "# 准备训练 (只训练 U-Net, 固定 VAE 和 Encoder, 因此下面就冻结其他两个模型的参数)\n",
    "# 冻结编码器模型的所有参数，使其在训练时不更新\n",
    "encoder.requires_grad_(False)\n",
    "# 冻结 VAE 模型的所有参数，使其在训练时不更新\n",
    "vae.requires_grad_(False)\n",
    "# 使 U-Net 模型的参数可训练，即在训练时更新\n",
    "unet.requires_grad_(True)\n",
    "\n",
    "# 设置编码器和 VAE 为评估模式 (即冻结它们的行为，防止 BatchNorm 和 Dropout 等操作影响)\n",
    "encoder.eval()\n",
    "vae.eval()\n",
    "# 设置 U-Net 为训练模式（启用 BatchNorm 和 Dropout 等训练时特有的操作）\n",
    "unet.train()\n",
    "\n",
    "# 将模型移到指定的设备（CPU 或 GPU）\n",
    "encoder.to(device)\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "# 设置优化器，只优化 U-Net 的参数\n",
    "# 使用 AdamW 优化器，学习率为 1e-5，beta1 和 beta2 是动量项的衰减率，weight_decay 是权重衰减系数，eps 是为数值稳定性添加的小数值\n",
    "optimizer = torch.optim.AdamW(unet.parameters(),\n",
    "                              lr=1e-5,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "# 使用均方误差 (MSE) 作为损失函数，用于衡量模型预测与目标之间的差异\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# 输出优化器和损失函数的配置\n",
    "optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0096, device='mps:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果模型预测的结果和真实答案非常接近，loss 就会很小；如果预测结果和真实答案相差很远，loss 就会很大\n",
    "def get_loss(data): # 入参是一批【图片数据】和【文本数据】\n",
    "    with torch.no_grad():\n",
    "        #文字编码\n",
    "        #[1, 77] -> [1, 77, 768]\n",
    "        out_encoder = encoder(data['input_ids'])\n",
    "\n",
    "        #抽取图像特征图\n",
    "        #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "        out_vae = vae.encoder(data['pixel_values'])\n",
    "        out_vae = vae.sample(out_vae)\n",
    "\n",
    "        #0.18215 = vae.config.scaling_factor\n",
    "        out_vae = out_vae * 0.18215\n",
    "\n",
    "    # 随机数,unet的计算目标\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "\n",
    "    # 往特征图中添加噪声\n",
    "    #1000 = scheduler.num_train_timesteps\n",
    "    #1 = batch size\n",
    "    # 添加随机噪声\n",
    "    noise_step = torch.randint(0, 1000, (1, )).long().to(device)\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    # 根据文字信息,把特征图中的噪声计算出来\n",
    "    out_unet = unet(out_vae=out_vae_noise,\n",
    "                    out_encoder=out_encoder,\n",
    "                    time=noise_step)\n",
    "\n",
    "    # 计算 mse loss\n",
    "    #[1, 4, 64, 64],[1, 4, 64, 64]\n",
    "    return criterion(out_unet, noise)\n",
    "\n",
    "\n",
    "get_loss({\n",
    "    'input_ids': torch.ones(1, 77, device=device).long(),\n",
    "    'pixel_values': torch.randn(1, 3, 512, 512, device=device)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.482576073292876\n",
      "10 103.78535157235456\n",
      "20 101.80729839394917\n",
      "30 97.18179716335726\n",
      "40 95.27005371931591\n",
      "50 93.52235668250069\n",
      "60 91.23645739001222\n",
      "70 85.15300659097556\n",
      "80 86.9126384588817\n",
      "90 81.5698299240612\n",
      "100 78.39926671008288\n",
      "110 75.35619182675873\n",
      "120 74.10794699968392\n",
      "130 71.74995687346382\n",
      "140 68.43087464362179\n",
      "150 63.701912391486985\n",
      "160 64.13840378901659\n",
      "170 59.5763320050537\n",
      "180 58.09928199512069\n",
      "190 54.098192290555744\n",
      "200 53.715583391978726\n",
      "210 49.144029857572605\n",
      "220 47.92628015137598\n",
      "230 45.43078308462282\n",
      "240 42.47678213690597\n",
      "250 42.57540527023593\n",
      "260 38.39479267646675\n",
      "270 36.950618647257215\n",
      "280 37.183194903125695\n",
      "290 33.844317765288\n",
      "300 32.76033932725841\n",
      "310 30.12229502759874\n",
      "320 30.798989617975167\n",
      "330 30.148435787028575\n",
      "340 27.67200335358939\n",
      "350 27.441537938197143\n",
      "360 26.668230306680925\n",
      "370 26.568718082347914\n",
      "380 24.796474545077217\n",
      "390 24.26365733845887\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory saves does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 训练结束后保存模型到指定路径\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(unet\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaves/unet.model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         loss_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 训练结束后保存模型到指定路径\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaves/unet.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/torch/serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory saves does not exist."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 👇👇 开始训练\n",
    "def train():\n",
    "    loss_sum = 0\n",
    "    # 🚀🚀 训练 400 个 epoch\n",
    "    for epoch in range(400):\n",
    "        # 遍历每一个 batch 的数据\n",
    "        for i, data in enumerate(loader):\n",
    "            # 计算损失并做反向传播（loss.backward），但不立即更新模型参数\n",
    "            loss = get_loss(data) / 4  # 每 4 个批次做一次参数的调整\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            # 每 4 个批次更新一次模型参数\n",
    "            if (epoch * len(loader) + i) % 4 == 0:\n",
    "                # 梯度裁剪，防止梯度爆炸\n",
    "                torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                # 更新模型参数\n",
    "                optimizer.step()\n",
    "                # 清零优化器的梯度\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # 每 10 个 epoch 输出一次累计的损失值并重置\n",
    "        if epoch % 10 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    # 训练结束后保存模型到指定路径\n",
    "    # 检查目录是否存在，如果不存在则创建\n",
    "    os.makedirs('saves', exist_ok=True)\n",
    "    torch.save(unet.to('cpu'), 'saves/unet.model')\n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "# 包装类\n",
    "class Model(PreTrainedModel):\n",
    "    # 定义配置类，这里使用预训练配置\n",
    "    config_class = PretrainedConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # 初始化父类的构造函数\n",
    "        super().__init__(config)\n",
    "        # 将 U-Net 模型加载到 CPU 上并作为类的一个属性\n",
    "        self.unet = unet.to('cpu')\n",
    "\n",
    "# 👉 保存模型到 Hugging Face Hub\n",
    "Model(PretrainedConfig()).push_to_hub(\n",
    "    # 指定要上传到 Hugging Face Hub 的模型库 ID\n",
    "    repo_id='lansinuote/diffsion_from_scratch.unet',\n",
    "    # 使用从文件中读取的认证令牌进行授权\n",
    "    use_auth_token=open('/root/hub_token.txt').read().strip()\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
