{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnet 模型的作用\n",
    "结合文本给图像降噪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 32, 32])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 定义残差连接层\n",
    "class Resnet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time = torch.nn.Sequential(\n",
    "            torch.nn.SiLU(),\n",
    "            torch.torch.nn.Linear(1280, dim_out),\n",
    "            torch.nn.Unflatten(dim=1, unflattened_size=(dim_out, 1, 1)),\n",
    "        )\n",
    "\n",
    "        self.s0 = torch.nn.Sequential(\n",
    "            torch.torch.nn.GroupNorm(num_groups=32,\n",
    "                                     num_channels=dim_in,\n",
    "                                     eps=1e-05,\n",
    "                                     affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.torch.nn.Conv2d(dim_in,\n",
    "                                  dim_out,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "        )\n",
    "\n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.torch.nn.GroupNorm(num_groups=32,\n",
    "                                     num_channels=dim_out,\n",
    "                                     eps=1e-05,\n",
    "                                     affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.torch.nn.Conv2d(dim_out,\n",
    "                                  dim_out,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1),\n",
    "        )\n",
    "\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.torch.nn.Conv2d(dim_in,\n",
    "                                             dim_out,\n",
    "                                             kernel_size=1,\n",
    "                                             stride=1,\n",
    "                                             padding=0)\n",
    "\n",
    "    # time 表示了下面 forward 中 x 里边【噪声】的比例\n",
    "    def forward(self, x, time):\n",
    "        # x -> [1, 320, 64, 64]\n",
    "        #time -> [1, 1280]\n",
    "\n",
    "        res = x\n",
    "\n",
    "        #[1, 1280] -> [1, 640, 1, 1]\n",
    "        time = self.time(time)\n",
    "\n",
    "        #[1, 320, 64, 64] -> [1, 640, 32, 32]\n",
    "        x = self.s0(x) + time\n",
    "\n",
    "        #维度不变\n",
    "        #[1, 640, 32, 32]\n",
    "        x = self.s1(x)\n",
    "\n",
    "        #[1, 320, 64, 64] -> [1, 640, 32, 32]\n",
    "        if self.res:\n",
    "            res = self.res(res)\n",
    "\n",
    "        #维度不变\n",
    "        #[1, 640, 32, 32]\n",
    "        x = res + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "Resnet(320, 640)(torch.randn(1, 320, 32, 32), torch.randn(1, 1280)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 320])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义 Unet 的注意力层 (计算 图 文 的注意力)\n",
    "class CrossAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_q, dim_kv):\n",
    "        #dim_q -> 320\n",
    "        #dim_kv -> 768\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_q = dim_q\n",
    "\n",
    "        # 👇 三个线性运算成为三个矩阵\n",
    "        self.q = torch.nn.Linear(dim_q, dim_q, bias=False)\n",
    "        self.k = torch.nn.Linear(dim_kv, dim_q, bias=False)\n",
    "        self.v = torch.nn.Linear(dim_kv, dim_q, bias=False)\n",
    "\n",
    "        self.out = torch.nn.Linear(dim_q, dim_q)\n",
    "\n",
    "    # 🔥 q 是图像数据, kv 是文本数据, 因为 q != kv, 因此这里计算就【⚠️不是自注意力】而是【相互注意力】!\n",
    "    def forward(self, q, kv):\n",
    "        #x -> [1, 4096, 320]\n",
    "        #kv -> [1, 77, 768]\n",
    "\n",
    "        # 🔨 拆分 q k v 形成多头注意力\n",
    "        #[1, 4096, 320] -> [1, 4096, 320]\n",
    "        q = self.q(q)\n",
    "        #[1, 77, 768] -> [1, 77, 320]\n",
    "        k = self.k(kv)\n",
    "        #[1, 77, 768] -> [1, 77, 320]\n",
    "        v = self.v(kv)\n",
    "\n",
    "        def reshape(x):\n",
    "            #x -> [1, 4096, 320]\n",
    "            b, lens, dim = x.shape\n",
    "\n",
    "            #[1, 4096, 320] -> [1, 4096, 8, 40]\n",
    "            x = x.reshape(b, lens, 8, dim // 8)\n",
    "\n",
    "            #[1, 4096, 8, 40] -> [1, 8, 4096, 40]\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            #[1, 8, 4096, 40] -> [8, 4096, 40]\n",
    "            x = x.reshape(b * 8, lens, dim // 8)\n",
    "\n",
    "            return x\n",
    "\n",
    "        #[1, 4096, 320] -> [8, 4096, 40]\n",
    "        q = reshape(q)\n",
    "        #[1, 77, 320] -> [8, 77, 40]\n",
    "        k = reshape(k)\n",
    "        #[1, 77, 320] -> [8, 77, 40]\n",
    "        v = reshape(v)\n",
    "\n",
    "        #[8, 4096, 40] * [8, 40, 77] -> [8, 4096, 77]\n",
    "        #atten = q.bmm(k.transpose(1, 2)) * (self.dim_q // 8)**-0.5\n",
    "\n",
    "        #从数学上是等价的,但是在实际计算时会产生很小的误差\n",
    "        atten = torch.baddbmm(\n",
    "            torch.empty(q.shape[0], q.shape[1], k.shape[1], device=q.device),\n",
    "            q,\n",
    "            k.transpose(1, 2),\n",
    "            beta=0,\n",
    "            alpha=(self.dim_q // 8)**-0.5,\n",
    "        )\n",
    "\n",
    "        atten = atten.softmax(dim=-1)\n",
    "\n",
    "        # [8, 4096, 77] * [8, 77, 40] -> [8, 4096, 40]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "    \n",
    "        def reshape(x):\n",
    "            #x -> [8, 4096, 40]\n",
    "            b, lens, dim = x.shape\n",
    "\n",
    "            #[8, 4096, 40] -> [1, 8, 4096, 40]\n",
    "            x = x.reshape(b // 8, 8, lens, dim)\n",
    "\n",
    "            #[1, 8, 4096, 40] -> [1, 4096, 8, 40]\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            #[1, 4096, 320]\n",
    "            x = x.reshape(b // 8, lens, dim * 8)\n",
    "\n",
    "            return x\n",
    "\n",
    "        # [8, 4096, 40] -> [1, 4096, 320]\n",
    "        # 👀 把多头注意力合成一个\n",
    "        atten = reshape(atten)\n",
    "\n",
    "        #[1, 4096, 320] -> [1, 4096, 320]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        return atten\n",
    "\n",
    "\n",
    "CrossAttention(320, 768)(torch.randn(1, 4096, 320), torch.randn(1, 77,\n",
    "                                                                768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transofrmer 层类\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        #in\n",
    "        self.norm_in = torch.nn.GroupNorm(num_groups=32,\n",
    "                                          num_channels=dim,\n",
    "                                          eps=1e-6,\n",
    "                                          affine=True)\n",
    "        self.cnn_in = torch.nn.Conv2d(dim,\n",
    "                                      dim,\n",
    "                                      kernel_size=1,\n",
    "                                      stride=1,\n",
    "                                      padding=0)\n",
    "\n",
    "        #atten\n",
    "        self.norm_atten0 = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.atten1 = CrossAttention(dim, dim)\n",
    "        self.norm_atten1 = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.atten2 = CrossAttention(dim, 768)\n",
    "\n",
    "        #act\n",
    "        self.norm_act = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.fc0 = torch.nn.Linear(dim, dim * 8)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.fc1 = torch.nn.Linear(dim * 4, dim)\n",
    "\n",
    "        #out\n",
    "        self.cnn_out = torch.nn.Conv2d(dim,\n",
    "                                       dim,\n",
    "                                       kernel_size=1,\n",
    "                                       stride=1,\n",
    "                                       padding=0)\n",
    "\n",
    "    # 🔥 q 是一个图像数据, kv 是一个文本数据\n",
    "    def forward(self, q, kv):\n",
    "        #q -> [1, 320, 64, 64]\n",
    "        #kv -> [1, 77, 768]\n",
    "        b, _, h, w = q.shape\n",
    "        res1 = q\n",
    "\n",
    "        #----in 输入部分----\n",
    "        # 先进行规范化，然后通过卷积层处理，维度保持不变\n",
    "        #[1, 320, 64, 64]\n",
    "        q = self.cnn_in(self.norm_in(q))\n",
    "\n",
    "        #[1, 320, 64, 64] -> [1, 64, 64, 320] -> [1, 4096, 320]\n",
    "        q = q.permute(0, 2, 3, 1).reshape(b, h * w, self.dim)\n",
    "\n",
    "        #---- atten ----\n",
    "         # 第一个注意力层处理输入数据，维度保持不变\n",
    "        #[1, 4096, 320]\n",
    "        q = self.atten1(q=self.norm_atten0(q), kv=self.norm_atten0(q)) + q\n",
    "        q = self.atten2(q=self.norm_atten1(q), kv=kv) + q\n",
    "\n",
    "\n",
    "        #---- act 激活函数部分 ----\n",
    "        # 先进行规范化，然后通过全连接层扩展维度\n",
    "        # [1, 4096, 320]\n",
    "        res2 = q\n",
    "        #[1, 4096, 320] -> [1, 4096, 2560]\n",
    "        q = self.fc0(self.norm_act(q))\n",
    "\n",
    "\n",
    "        # 切分和激活操作\n",
    "        # 1280\n",
    "        d = q.shape[2] // 2\n",
    "\n",
    "        # 通过另一个全连接层减少维度\n",
    "        #[1, 4096, 1280] * [1, 4096, 1280] -> [1, 4096, 1280]\n",
    "        q = q[:, :, :d] * self.act(q[:, :, d:])\n",
    "\n",
    "        #[1, 4096, 1280] -> [1, 4096, 320]\n",
    "        q = self.fc1(q) + res2\n",
    "\n",
    "\n",
    "\n",
    "        #----out 输出部分 (一个 CNN 的运算加上残差的连接）----\n",
    "        # 将数据恢复成图像的维度，维度保持不变\n",
    "        #[1, 4096, 320] -> [1, 64, 64, 320] -> [1, 320, 64, 64]\n",
    "        q = q.reshape(b, h, w, self.dim).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # 通过卷积层处理输出，加上残差连接, 维度不变\n",
    "        # [1, 320, 64, 64]\n",
    "        q = self.cnn_out(q) + res1\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "Transformer(320)(torch.randn(1, 320, 64, 64), torch.randn(1, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 16, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Down 层\n",
    "# DownBlock 层类\n",
    "class DownBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # 初始化两个 Transformer 层和两个 ResNet 层\n",
    "        self.tf0 = Transformer(dim_out)  # 用于处理输入数据的第一个 Transformer 层\n",
    "        self.res0 = Resnet(dim_in, dim_out)  # 用于输入数据的第一个 ResNet 层\n",
    "\n",
    "        self.tf1 = Transformer(dim_out)  # 用于处理数据的第二个 Transformer 层\n",
    "        self.res1 = Resnet(dim_out, dim_out)  # 用于数据的第二个 ResNet 层\n",
    "\n",
    "        # 输出卷积层，用于下采样，将空间维度减少一半\n",
    "        self.out = torch.nn.Conv2d(dim_out,\n",
    "                                   dim_out,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=2,\n",
    "                                   padding=1)\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time):\n",
    "        outs = []\n",
    "\n",
    "        # 第一阶段处理\n",
    "        out_vae = self.res0(out_vae, time)  # 通过第一个 ResNet 层处理输入数据\n",
    "        out_vae = self.tf0(out_vae, out_encoder)  # 通过第一个 Transformer 层处理数据\n",
    "        outs.append(out_vae)  # 将结果添加到输出列表中\n",
    "\n",
    "        # 第二阶段处理\n",
    "        out_vae = self.res1(out_vae, time)  # 通过第二个 ResNet 层处理数据\n",
    "        out_vae = self.tf1(out_vae, out_encoder)  # 通过第二个 Transformer 层处理数据\n",
    "        outs.append(out_vae)  # 将结果添加到输出列表中\n",
    "\n",
    "        # 输出阶段\n",
    "        out_vae = self.out(out_vae)  # 使用卷积层对数据进行下采样\n",
    "        outs.append(out_vae)  # 将最终结果添加到输出列表中\n",
    "\n",
    "        # 返回处理后的数据和所有阶段的结果\n",
    "        return out_vae, outs\n",
    "\n",
    "\n",
    "DownBlock(320, 640)(torch.randn(1, 320, 32, 32), torch.randn(1, 77, 768),\n",
    "                    torch.randn(1, 1280))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Up 层\n",
    "class UpBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, dim_prev, add_up):\n",
    "        super().__init__()\n",
    "\n",
    "        # 初始化三个 ResNet 层和三个 Transformer 层\n",
    "        self.res0 = Resnet(dim_out + dim_prev, dim_out)  # 处理合并后的输入数据\n",
    "        self.res1 = Resnet(dim_out + dim_out, dim_out)  # 处理合并后的数据\n",
    "        self.res2 = Resnet(dim_in + dim_out, dim_out)  # 处理合并后的数据\n",
    "\n",
    "        self.tf0 = Transformer(dim_out)  # 用于处理数据的第一个 Transformer 层\n",
    "        self.tf1 = Transformer(dim_out)  # 用于处理数据的第二个 Transformer 层\n",
    "        self.tf2 = Transformer(dim_out)  # 用于处理数据的第三个 Transformer 层\n",
    "\n",
    "        # 输出层，用于上采样并进一步处理数据\n",
    "        self.out = None\n",
    "        if add_up:\n",
    "            self.out = torch.nn.Sequential(\n",
    "                torch.nn.Upsample(scale_factor=2, mode='nearest'),  # 上采样，将数据的空间尺寸扩大一倍\n",
    "                torch.nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1),  # 进一步处理上采样后的数据\n",
    "            )\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time, out_down):\n",
    "        # 第一阶段处理\n",
    "        # 合并当前层的输出数据 `out_vae` 和来自下采样层的数据 `out_down`，然后通过第一个 ResNet 层处理\n",
    "        out_vae = self.res0(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        out_vae = self.tf0(out_vae, out_encoder)  # 通过第一个 Transformer 层处理数据\n",
    "\n",
    "        # 第二阶段处理\n",
    "        # 合并当前层的输出数据 `out_vae` 和来自下采样层的数据 `out_down`，然后通过第二个 ResNet 层处理\n",
    "        out_vae = self.res1(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        out_vae = self.tf1(out_vae, out_encoder)  # 通过第二个 Transformer 层处理数据\n",
    "\n",
    "        # 第三阶段处理\n",
    "        # 合并当前层的输出数据 `out_vae` 和来自下采样层的数据 `out_down`，然后通过第三个 ResNet 层处理\n",
    "        out_vae = self.res2(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        out_vae = self.tf2(out_vae, out_encoder)  # 通过第三个 Transformer 层处理数据\n",
    "\n",
    "        # 如果需要上采样，则应用上采样层\n",
    "        if self.out:\n",
    "            out_vae = self.out(out_vae)\n",
    "\n",
    "        return out_vae\n",
    "\n",
    "\n",
    "# 测试\n",
    "UpBlock(320, 640, 1280, True)(torch.randn(1, 1280, 32, 32),\n",
    "                              torch.randn(1, 77, 768), torch.randn(1, 1280), [\n",
    "                                  torch.randn(1, 320, 32, 32),\n",
    "                                  torch.randn(1, 640, 32, 32),\n",
    "                                  torch.randn(1, 640, 32, 32)\n",
    "                              ]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🌟 Unet 模型的类 【核心】\n",
    "class UNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #in\n",
    "        self.in_vae = torch.nn.Conv2d(4, 320, kernel_size=3, padding=1)\n",
    "\n",
    "        self.in_time = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 1280),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(1280, 1280),\n",
    "        )\n",
    "\n",
    "        #down\n",
    "        self.down_block0 = DownBlock(320, 320)\n",
    "        self.down_block1 = DownBlock(320, 640)\n",
    "        self.down_block2 = DownBlock(640, 1280)\n",
    "\n",
    "        self.down_res0 = Resnet(1280, 1280)\n",
    "        self.down_res1 = Resnet(1280, 1280)\n",
    "\n",
    "        #mid\n",
    "        self.mid_res0 = Resnet(1280, 1280)\n",
    "        self.mid_tf = Transformer(1280)\n",
    "        self.mid_res1 = Resnet(1280, 1280)\n",
    "\n",
    "        #up\n",
    "        self.up_res0 = Resnet(2560, 1280)\n",
    "        self.up_res1 = Resnet(2560, 1280)\n",
    "        self.up_res2 = Resnet(2560, 1280)\n",
    "\n",
    "        self.up_in = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            torch.nn.Conv2d(1280, 1280, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.up_block0 = UpBlock(640, 1280, 1280, True)\n",
    "        self.up_block1 = UpBlock(320, 640, 1280, True)\n",
    "        self.up_block2 = UpBlock(320, 320, 640, False)\n",
    "\n",
    "        #out\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_channels=320, num_groups=32, eps=1e-5),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(320, 4, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    # 🔥 out_vae 是一个特征图的形状\n",
    "    def forward(self, out_vae, out_encoder, time):\n",
    "        #out_vae -> [1, 4, 64, 64]     特征图的形状\n",
    "        #out_encoder -> [1, 77, 768]   文本数据的形状\n",
    "        #time -> [1]\n",
    "\n",
    "        #-------- in------------------------------------------------------------------------\n",
    "        #[1, 4, 64, 64] -> [1, 320, 64, 64]\n",
    "        out_vae = self.in_vae(out_vae) # CNN 运算\n",
    "\n",
    "        def get_time_embed(t):\n",
    "            #-9.210340371976184 = -math.log(10000)\n",
    "            e = torch.arange(160) * -9.210340371976184 / 160\n",
    "            e = e.exp().to(t.device) * t\n",
    "\n",
    "            #[160+160] -> [320] -> [1, 320]\n",
    "            e = torch.cat([e.cos(), e.sin()]).unsqueeze(dim=0)\n",
    "\n",
    "            return e\n",
    "\n",
    "        # 对 time 进行运算, 变成一个 1280 的向量\n",
    "        #[1] -> [1, 320]\n",
    "        time = get_time_embed(time)\n",
    "        #[1, 320] -> [1, 1280]\n",
    "        time = self.in_time(time)\n",
    "\n",
    "        #---------- down 层的计算 : 一层层的计算图文的注意力 ------------------------------------------------------------------------\n",
    "        #[1, 320, 64, 64]\n",
    "        #[1, 320, 64, 64]\n",
    "        #[1, 320, 64, 64]\n",
    "        #[1, 320, 32, 32]\n",
    "        #[1, 640, 32, 32]\n",
    "        #[1, 640, 32, 32]\n",
    "        #[1, 640, 16, 16]\n",
    "        #[1, 1280, 16, 16]\n",
    "        #[1, 1280, 16, 16]\n",
    "        #[1, 1280, 8, 8]\n",
    "        #[1, 1280, 8, 8]\n",
    "        #[1, 1280, 8, 8]\n",
    "        out_down = [out_vae]\n",
    "\n",
    "        #[1, 320, 64, 64],[1, 77, 768],[1, 1280] -> [1, 320, 32, 32]\n",
    "        #out -> [1, 320, 64, 64],[1, 320, 64, 64][1, 320, 32, 32]\n",
    "        out_vae, out = self.down_block0(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[1, 320, 32, 32],[1, 77, 768],[1, 1280] -> [1, 640, 16, 16]\n",
    "        #out -> [1, 640, 32, 32],[1, 640, 32, 32],[1, 640, 16, 16]\n",
    "        out_vae, out = self.down_block1(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[1, 640, 16, 16],[1, 77, 768],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        #out -> [1, 1280, 16, 16],[1, 1280, 16, 16],[1, 1280, 8, 8]\n",
    "        out_vae, out = self.down_block2(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.down_res0(out_vae, time)\n",
    "        out_down.append(out_vae)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.down_res1(out_vae, time)\n",
    "        out_down.append(out_vae)\n",
    "\n",
    "        #---- mid 层 : 也是计算图文的注意力----\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.mid_res0(out_vae, time)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 77, 768] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.mid_tf(out_vae, out_encoder)\n",
    "\n",
    "        #[1, 1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.mid_res1(out_vae, time)\n",
    "\n",
    "\n",
    "        # ------------- up 层计算 : 跨层连接 dwon ------------------------------------------------------------------------\n",
    "        #[1, 1280+1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.up_res0(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                               time)\n",
    "\n",
    "        #[1, 1280+1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.up_res1(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                               time)\n",
    "\n",
    "        #[1, 1280+1280, 8, 8],[1, 1280] -> [1, 1280, 8, 8]\n",
    "        out_vae = self.up_res2(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                               time)\n",
    "\n",
    "        #[1, 1280, 8, 8] -> [1, 1280, 16, 16]\n",
    "        out_vae = self.up_in(out_vae)\n",
    "\n",
    "        #[1, 1280, 16, 16],[1, 77, 768],[1, 1280] -> [1, 1280, 32, 32]\n",
    "        #out_down -> [1, 640, 16, 16],[1, 1280, 16, 16],[1, 1280, 16, 16]\n",
    "        out_vae = self.up_block0(out_vae=out_vae,\n",
    "                                 out_encoder=out_encoder,\n",
    "                                 time=time,\n",
    "                                 out_down=out_down)\n",
    "\n",
    "        #[1, 1280, 32, 32],[1, 77, 768],[1, 1280] -> [1, 640, 64, 64]\n",
    "        #out_down -> [1, 320, 32, 32],[1, 640, 32, 32],[1, 640, 32, 32]\n",
    "        out_vae = self.up_block1(out_vae=out_vae,\n",
    "                                 out_encoder=out_encoder,\n",
    "                                 time=time,\n",
    "                                 out_down=out_down)\n",
    "\n",
    "        #[1, 640, 64, 64],[1, 77, 768],[1, 1280] -> [1, 320, 64, 64]\n",
    "        #out_down -> [1, 320, 64, 64],[1, 320, 64, 64],[1, 320, 64, 64]\n",
    "        out_vae = self.up_block2(out_vae=out_vae,\n",
    "                                 out_encoder=out_encoder,\n",
    "                                 time=time,\n",
    "                                 out_down=out_down)\n",
    "\n",
    "        #------ out 层计算 : 卷积的输出 ----------------------------------------------------------------------------\n",
    "        #[1, 320, 64, 64] -> [1, 4, 64, 64]\n",
    "        out_vae = self.out(out_vae)\n",
    "\n",
    "        return out_vae\n",
    "\n",
    "\n",
    "UNet()(torch.randn(2, 4, 64, 64), torch.randn(2, 77, 768),\n",
    "       torch.LongTensor([26])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用【预训练的模型】初始化数据\n",
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "#加载预训练模型的参数\n",
    "params = UNet2DConditionModel.from_pretrained(\n",
    "    'model/diffsion_from_scratch.params', subfolder='unet')\n",
    "\n",
    "unet = UNet()\n",
    "\n",
    "# 加载 in 的参数\n",
    "unet.in_vae.load_state_dict(params.conv_in.state_dict())\n",
    "unet.in_time[0].load_state_dict(params.time_embedding.linear_1.state_dict())\n",
    "unet.in_time[2].load_state_dict(params.time_embedding.linear_2.state_dict())\n",
    "\n",
    "\n",
    "# 加载 down 的参数\n",
    "def load_tf(model, param):\n",
    "    model.norm_in.load_state_dict(param.norm.state_dict())\n",
    "    model.cnn_in.load_state_dict(param.proj_in.state_dict())\n",
    "\n",
    "    model.atten1.q.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_q.state_dict())\n",
    "    model.atten1.k.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_k.state_dict())\n",
    "    model.atten1.v.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_v.state_dict())\n",
    "    model.atten1.out.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_out[0].state_dict())\n",
    "\n",
    "    model.atten2.q.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_q.state_dict())\n",
    "    model.atten2.k.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_k.state_dict())\n",
    "    model.atten2.v.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_v.state_dict())\n",
    "    model.atten2.out.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_out[0].state_dict())\n",
    "\n",
    "    model.fc0.load_state_dict(\n",
    "        param.transformer_blocks[0].ff.net[0].proj.state_dict())\n",
    "\n",
    "    model.fc1.load_state_dict(\n",
    "        param.transformer_blocks[0].ff.net[2].state_dict())\n",
    "\n",
    "    model.norm_atten0.load_state_dict(\n",
    "        param.transformer_blocks[0].norm1.state_dict())\n",
    "    model.norm_atten1.load_state_dict(\n",
    "        param.transformer_blocks[0].norm2.state_dict())\n",
    "    model.norm_act.load_state_dict(\n",
    "        param.transformer_blocks[0].norm3.state_dict())\n",
    "\n",
    "    model.cnn_out.load_state_dict(param.proj_out.state_dict())\n",
    "\n",
    "\n",
    "def load_res(model, param):\n",
    "    model.time[1].load_state_dict(param.time_emb_proj.state_dict())\n",
    "\n",
    "    model.s0[0].load_state_dict(param.norm1.state_dict())\n",
    "    model.s0[2].load_state_dict(param.conv1.state_dict())\n",
    "\n",
    "    model.s1[0].load_state_dict(param.norm2.state_dict())\n",
    "    model.s1[2].load_state_dict(param.conv2.state_dict())\n",
    "\n",
    "    if isinstance(model.res, torch.nn.Module):\n",
    "        model.res.load_state_dict(param.conv_shortcut.state_dict())\n",
    "\n",
    "\n",
    "def load_down_block(model, param):\n",
    "    load_tf(model.tf0, param.attentions[0])\n",
    "    load_tf(model.tf1, param.attentions[1])\n",
    "\n",
    "    load_res(model.res0, param.resnets[0])\n",
    "    load_res(model.res1, param.resnets[1])\n",
    "\n",
    "    model.out.load_state_dict(param.downsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "load_down_block(unet.down_block0, params.down_blocks[0])\n",
    "load_down_block(unet.down_block1, params.down_blocks[1])\n",
    "load_down_block(unet.down_block2, params.down_blocks[2])\n",
    "\n",
    "load_res(unet.down_res0, params.down_blocks[3].resnets[0])\n",
    "load_res(unet.down_res1, params.down_blocks[3].resnets[1])\n",
    "\n",
    "# 加载 mid 的数据\n",
    "load_tf(unet.mid_tf, params.mid_block.attentions[0])\n",
    "load_res(unet.mid_res0, params.mid_block.resnets[0])\n",
    "load_res(unet.mid_res1, params.mid_block.resnets[1])\n",
    "\n",
    "# 加载 up 的数据\n",
    "load_res(unet.up_res0, params.up_blocks[0].resnets[0])\n",
    "load_res(unet.up_res1, params.up_blocks[0].resnets[1])\n",
    "load_res(unet.up_res2, params.up_blocks[0].resnets[2])\n",
    "unet.up_in[1].load_state_dict(\n",
    "    params.up_blocks[0].upsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "def load_up_block(model, param):\n",
    "    load_tf(model.tf0, param.attentions[0])\n",
    "    load_tf(model.tf1, param.attentions[1])\n",
    "    load_tf(model.tf2, param.attentions[2])\n",
    "\n",
    "    load_res(model.res0, param.resnets[0])\n",
    "    load_res(model.res1, param.resnets[1])\n",
    "    load_res(model.res2, param.resnets[2])\n",
    "\n",
    "    if isinstance(model.out, torch.nn.Module):\n",
    "        model.out[1].load_state_dict(param.upsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "load_up_block(unet.up_block0, params.up_blocks[1])\n",
    "load_up_block(unet.up_block1, params.up_blocks[2])\n",
    "load_up_block(unet.up_block2, params.up_blocks[3])\n",
    "\n",
    "# 加载 out 的数据\n",
    "unet.out[0].load_state_dict(params.conv_norm_out.state_dict())\n",
    "unet.out[2].load_state_dict(params.conv_out.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 虚拟一批数据并检查两个模型的输出结果\n",
    "out_vae = torch.randn(1, 4, 64, 64)\n",
    "out_encoder = torch.randn(1, 77, 768)\n",
    "time = torch.LongTensor([26])\n",
    "\n",
    "a = unet(out_vae=out_vae, out_encoder=out_encoder, time=time)\n",
    "b = params(out_vae, time, out_encoder).sample\n",
    "\n",
    "(a == b).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
