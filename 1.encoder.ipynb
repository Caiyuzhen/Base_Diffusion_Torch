{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¿è¡Œç¯å¢ƒ\n",
    "`diffusers == 0.121`\n",
    "\n",
    "# Encoder çš„ä½œç”¨\n",
    "- ç‰¹å¾æå–ï¼š\n",
    "  - Encoder è´Ÿè´£ä»åŸå§‹è¾“å…¥æ•°æ®ä¸­æå–é«˜å±‚æ¬¡çš„ç‰¹å¾ã€‚å¯¹äºå›¾åƒæ¥è¯´ï¼ŒEncoder å¯èƒ½ä¼šæå–å‡ºè¾¹ç¼˜ã€çº¹ç†ç­‰ç‰¹å¾ï¼›å¯¹äºæ–‡æœ¬æ¥è¯´ï¼Œå®ƒå¯èƒ½ä¼šæå–å‡ºå¥æ³•ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚è¿™ç§ç‰¹å¾æå–é€šå¸¸é€šè¿‡å¤šä¸ªå·ç§¯å±‚ï¼ˆå¯¹äºå›¾åƒï¼‰æˆ–å¤šä¸ª LSTM/Transformer å±‚ï¼ˆå¯¹äºæ–‡æœ¬ï¼‰æ¥å®ç°ã€‚\n",
    "\n",
    "- é™ç»´å’Œå‹ç¼©ï¼š\n",
    "  - Encoder é€šå¸¸å°†é«˜ç»´çš„è¾“å…¥æ•°æ®è½¬æ¢ä¸ºä½ç»´çš„æ½œåœ¨è¡¨ç¤ºï¼ˆlatent representationï¼‰ã€‚è¿™ç§æ½œåœ¨è¡¨ç¤ºä¿ç•™äº†è¾“å…¥æ•°æ®çš„æœ€é‡è¦ä¿¡æ¯ï¼Œä½†ç»´åº¦è¾ƒä½ï¼Œæœ‰åŠ©äºå‡å°‘è®¡ç®—å¤æ‚æ€§å’Œå­˜å‚¨éœ€æ±‚ã€‚åœ¨è‡ªç¼–ç å™¨ä¸­ï¼Œç¼–ç å™¨çš„è¾“å‡ºé€šå¸¸æ˜¯æ½œåœ¨ç©ºé—´ï¼ˆlatent spaceï¼‰ä¸­çš„ä¸€ä¸ªå‘é‡ï¼Œè¿™ä¸ªå‘é‡æ˜¯æ•°æ®åœ¨è¾ƒä½ç»´åº¦ç©ºé—´ä¸­çš„å‹ç¼©è¡¨ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# åµŒå…¥å±‚\n",
    "class Embed(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # å› ä¸ºç»§æ‰¿äº†çˆ¶ç±», æ‰€ä»¥è¿™é‡Œæ˜¯åšä¸ªåˆå§‹åŒ–(ğŸ”¥ èƒ½å¤ŸæŠŠçˆ¶ç±»çš„å±æ€§ç»§æ‰¿è¿‡æ¥)\n",
    "        self.embed = torch.nn.Embedding(49408, 768)  # åˆ›å»ºè¯åµŒå…¥å±‚ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º 49408ï¼ŒåµŒå…¥ç»´åº¦ä¸º 768\n",
    "        self.pos_embed = torch.nn.Embedding(77, 768)  # åˆ›å»ºä½ç½®åµŒå…¥å±‚ï¼Œä½ç½®æ•°é‡ä¸º 77ï¼ŒåµŒå…¥ç»´åº¦ä¸º 768\n",
    "        self.register_buffer('pos_ids', torch.arange(77).unsqueeze(dim=0))  # æ³¨å†Œä½ç½®ç´¢å¼•ç¼“å†²åŒºï¼ŒèŒƒå›´ä¸º [0, 76]ï¼Œå½¢çŠ¶ä¸º [1, 77]\n",
    "        \n",
    "    def forward(self, input_ids): # å®šä¹‰å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œinput_ids æ˜¯è¾“å…¥çš„è¯ç´¢å¼•\n",
    "        \n",
    "        # å½¢çŠ¶ä» [batch_size, 77] -> [batch_size, 77, 768]\n",
    "        embed = self.embed(input_ids) # âœï¸ è¯ç¼–ç ï¼ˆæç¤ºè¯ï¼‰\n",
    "        \n",
    "        # å¯¹ä½ç½®ç´¢å¼•è¿›è¡Œä½ç½®ç¼–ç \n",
    "        # å½¢çŠ¶ä» [1, 77] -> [batch_size, 77, 768]\n",
    "        pos_embed = self.pos_embed(self.pos_ids) # ğŸš— ä½ç½®ç¼–ç ï¼ˆå›¾ç‰‡ï¼‰\n",
    "        \n",
    "        # [batch_size, 77, 768]\n",
    "        # å°†è¯åµŒå…¥å’Œä½ç½®åµŒå…¥ç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„åµŒå…¥è¡¨ç¤º\n",
    "        \n",
    "        # è¾“å‡ºå½¢çŠ¶ä¸º [batch_size, 77, 768]\n",
    "        return embed + pos_embed\n",
    "    \n",
    "# å®ä¾‹åŒ– Embed ç±»ï¼Œå¹¶å¯¹è¾“å…¥ä¸º [2, 77] çš„å¼ é‡è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶\n",
    "Embed()(torch.ones(2, 77).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# æ³¨æ„åŠ›å±‚\n",
    "class Atten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # å¤šå¤´çº¿æ€§\n",
    "        self.q = torch.nn.Linear(768, 768)\n",
    "        self.k = torch.nn.Linear(768, 768)\n",
    "        self.v = torch.nn.Linear(768, 768)\n",
    "        self.out = torch.nn.Linear(768, 768)\n",
    "\n",
    "        \n",
    "     # å®šä¹‰å‘å‰ä¼ æ’­çš„é€»è¾‘\n",
    "    def forward(self, x):\n",
    "        # x -> [batch_size, 77, 768]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # ç»´åº¦ä¸å˜\n",
    "        # [batch_size, 77, 768]\n",
    "        q = self.q(x) * 0.125  # é€šè¿‡æŸ¥è¯¢çº¿æ€§å±‚è®¡ç®—æŸ¥è¯¢å‘é‡ï¼Œå¹¶ç¼©æ”¾ï¼Œå½¢çŠ¶ä¸º [batch_size, 77, 768]\n",
    "        k = self.k(x)  # é€šè¿‡é”®çº¿æ€§å±‚è®¡ç®—é”®å‘é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, 77, 768]\n",
    "        v = self.v(x)  # é€šè¿‡å€¼çº¿æ€§å±‚è®¡ç®—å€¼å‘é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, 77, 768]\n",
    "        \n",
    "        # æ‹†åˆ†æ³¨æ„åŠ›å¤´ (ä¸€åˆ†ä¸ºä¸‰, æŠ½å–æ¯ä¸ªç‰¹å¾çš„ç‰‡æ®µ)\n",
    "        # ç»å…¸çš„ Transformer æ¨¡å‹ï¼ˆå¦‚ BERTã€GPTï¼‰é€šå¸¸ä½¿ç”¨ 12 ä¸ªæ³¨æ„åŠ›å¤´, æ¯ä¸ªå¤´çš„ç»´åº¦ä¸º 64\n",
    "        # [batch_size, 77, 768] -> [batch_size, 77, 12, 64] -> [batch_size, 12, 77, 64] -> [batch_size * 12, 77, 64]\n",
    "        q = q.reshape(batch_size, 77, 12, 64).transpose(1, 2).reshape(batch_size * 12, 77, 64)\n",
    "        k = k.reshape(batch_size, 77, 12, 64).transpose(1, 2).reshape(batch_size * 12, 77, 64)\n",
    "        v = v.reshape(batch_size, 77, 12, 64).transpose(1, 2).reshape(batch_size * 12, 77, 64)\n",
    "        \n",
    "        # æ³¨æ„åŠ›è®¡ç®—é€šè¿‡çŸ©é˜µä¹˜æ³•å®ç° => è®¡ç®—æŸ¥è¯¢ä¸é”®çš„ç›¸ä¼¼æ€§\n",
    "        # [batch_size * 12, 77, 64] @ [batch_size * 12, 64, 77] -> [batch_size * 12, 77, 77]\n",
    "        attention = torch.bmm(q, k.transpose(1, 2))\n",
    "        \n",
    "        # é‡æ–°è°ƒæ•´å½¢çŠ¶ï¼Œè¿”å›å½¢çŠ¶ä¸º [batch_size, 12, 77, 77] çš„æ³¨æ„åŠ›çŸ©é˜µ\n",
    "        attention = attention.reshape(batch_size, 12, 77, 77)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Mask (é€‰æ‹©æ€§åœ°å…è®¸æˆ–é˜»æ­¢æ³¨æ„åŠ›, å‘åæ³¨æ„åŠ› mask, åé¢åªèƒ½æ³¨æ„åˆ°åä¸€ä¸ª) => ç”Ÿæˆä¸€ä¸ªç”¨äºæ©ç ï¼ˆMaskï¼‰çš„å¼ é‡ï¼Œé€‚ç”¨äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„åºåˆ—å¤„ç†        \n",
    "        def get_mask(batch_size):\n",
    "            mask = torch.empty(batch_size, 77, 77)\n",
    "\n",
    "            # ä¸Šä¸‰è§’å¯¹è§’çº¿ä»¥ä¸Šçš„éƒ¨åˆ†ç½®ä¸ºè´Ÿæ— ç©·\n",
    "            mask.fill_(-float('inf'))\n",
    "\n",
    "            # å¯¹è§’çº¿å’Œä»¥ä¸‹çš„ä½ç½®ä¸º0\n",
    "            mask.triu_(1)\n",
    "\n",
    "            return mask.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        # ğŸ‘‡ ä½¿ç”¨ mask æ¥é®ç›–æ³¨æ„åŠ›\n",
    "        # [batch_size, 12, 77, 77] + [batch_size, 1, 77, 77] -> [batch_size, 12, 77, 77]\n",
    "        attention = attention + get_mask(attention.shape[0]).to(attention.device)\n",
    "\n",
    "        # [batch_size, 12, 77, 77] -> [batch_size * 12, 77, 77]\n",
    "        attention = attention.reshape(batch_size * 12, 77, 77)\n",
    "\n",
    "        # è®¡ç®— softmax, è¢« mask çš„éƒ¨åˆ†å€¼å‹ç¼©ä¸º 0, å› ä¸º è´Ÿæ— ç©·, æ‰€ä»¥æ¯ä¸ªè¯åªèƒ½æ³¨æ„åˆ°å‰ä¸€ä¸ªè¯\n",
    "        attention = attention.softmax(dim=-1)\n",
    "\n",
    "        # è®¡ç®—å’Œ v çš„ä¹˜ç§¯\n",
    "        attention = torch.bmm(attention, v)\n",
    "\n",
    "        # [batch_size * 12, 77, 64] -> [batch_size, 12, 77, 64] -> [batch_size, 77, 768]\n",
    "        attention = attention.reshape(batch_size, 12, 77, 64).transpose(1, 2).reshape(batch_size, 77, 768)\n",
    "\n",
    "        # çº¿æ€§è¾“å‡º, ç»´åº¦ä¸å˜\n",
    "        # [batch_size, 77, 768]\n",
    "        return self.out(attention)\n",
    "\n",
    "Atten()(torch.randn(2, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ç¼–ç å™¨å±‚\n",
    "class ClipEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç»§æ‰¿çˆ¶ç±»çš„å±æ€§å’Œæ–¹æ³•\n",
    "        \n",
    "        # ğŸ‘‡ ä½¿ç”¨ Sequential æ¥  æŒ‰é¡ºåº  ç»„åˆå¤šä¸ªç¥ç»ç½‘ç»œå±‚\n",
    "        # ç¬¬ä¸€å±‚åºåˆ—åŒ–ç½‘ç»œï¼šåŒ…æ‹¬å±‚å½’ä¸€åŒ–å’Œè‡ªå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶\n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),  # å½’ä¸€åŒ–è¾“å…¥ï¼Œä¿æŒç¨³å®šè®­ç»ƒ\n",
    "            Atten(),  # è‡ªå®šä¹‰æ³¨æ„åŠ›å±‚ï¼Œå¤„ç†è¾“å…¥æ•°æ®\n",
    "        )\n",
    "        \n",
    "        # ç¬¬äºŒå±‚åºåˆ—åŒ–ç½‘ç»œï¼šåŒ…æ‹¬å±‚å½’ä¸€åŒ–å’Œå…¨è¿æ¥å±‚\n",
    "        self.s2 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),  # å†æ¬¡è¿›è¡Œå½’ä¸€åŒ–\n",
    "            torch.nn.Linear(768, 3072)  # å°†è¾“å…¥ä» 768 ç»´æ˜ å°„åˆ° 3072 ç»´\n",
    "        )\n",
    "        \n",
    "        # ç¬¬ä¸‰å±‚ï¼šçº¿æ€§å˜æ¢ï¼Œå°†ç»´åº¦ä» 3072 å†æ¬¡ç¼©å›åˆ° 768\n",
    "        self.s3 = torch.nn.Linear(3072, 768)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # è¾“å…¥ x çš„å½¢çŠ¶ä¸º [2, 77, 768]\n",
    "        # é€šè¿‡ç¬¬ä¸€å±‚å¤„ç†ï¼Œç»´åº¦ä¿æŒä¸å˜ -> [2, 77, 768]\n",
    "        x = x + self.s1(x)  # æ®‹å·®è¿æ¥ï¼ˆResNet é£æ ¼ï¼‰ï¼Œå°†åŸå§‹è¾“å…¥ä¸æ³¨æ„åŠ›å±‚è¾“å‡ºç›¸åŠ \n",
    "        \n",
    "        # ä¿å­˜å½“å‰çš„ x å€¼ï¼Œç”¨äºåé¢çš„æ®‹å·®è¿æ¥\n",
    "        res = x  # ä¿å­˜ç¬¬ä¸€å±‚çš„è¾“å‡ºï¼Œç”¨äºæ®‹å·®è¿æ¥\n",
    "        \n",
    "        # é€šè¿‡ç¬¬äºŒå±‚å¤„ç†ï¼Œç»´åº¦ä» [2, 77, 768] -> [2, 77, 3072]\n",
    "        x = self.s2(x)\n",
    "        \n",
    "        # ä½¿ç”¨æ¿€æ´»å‡½æ•°å¤„ç† xï¼Œç»´åº¦ä¸å˜\n",
    "        x = x * (x * 1.702).sigmoid() \n",
    "        \n",
    "        # é€šè¿‡ç¬¬ä¸‰å±‚çº¿æ€§å˜æ¢ï¼Œå°†ç»´åº¦ä» [2, 77, 3072] ç¼©å›åˆ° [2, 77, 768]\n",
    "        # å°†ç»“æœä¸ä¹‹å‰ä¿å­˜çš„ res ç›¸åŠ ï¼Œå½¢æˆæœ€ç»ˆè¾“å‡º\n",
    "        return res + self.s3(x)  # æ®‹å·®è¿æ¥ï¼Œå°†åŸå§‹è¾“å…¥ä¸å¤„ç†åçš„è¾“å‡ºç›¸åŠ \n",
    "    \n",
    "# æµ‹è¯• ClipEncoder ç±»çš„åŠŸèƒ½\n",
    "# print(ClipEncoder()(torch.randn(2, 77, 768)).shape)  # è¾“å‡ºå¼ é‡çš„å½¢çŠ¶åº”è¯¥ä¸º [2, 77, 768]\n",
    "ClipEncoder()(torch.randn(2, 77, 768)).shape  # è¾“å‡ºå¼ é‡çš„å½¢çŠ¶åº”è¯¥ä¸º [2, 77, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸»æ¨¡å‹\n",
    "encoder = torch.nn.Sequential( # ğŸ‘ˆ ä½¿ç”¨ Sequential æ¥  æŒ‰é¡ºåº  ç»„åˆå¤šä¸ªç¥ç»ç½‘ç»œå±‚\n",
    "    Embed(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    torch.nn.LayerNorm(768)\n",
    ")\n",
    "\n",
    "# ğŸ‘‡ å¯¹æ¨¡å‹è¿›è¡Œè¯•ç®—, é¢„æœŸè¾“å‡º torch.Size([2, 77, 768])\n",
    "encoder(torch.ones(2, 77).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'diffsion_from_scratch.params', 'diffsion_from_scratch.unet']\n",
      "âœ… Model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ğŸ‘‹ åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "from transformers import CLIPTextModel\n",
    "import os\n",
    "\n",
    "\n",
    "print(os.listdir('model/'))\n",
    "\n",
    "#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°\n",
    "try:\n",
    "    params = CLIPTextModel.from_pretrained(\n",
    "        'model/diffsion_from_scratch.params', subfolder='text_encoder')\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error loading model:\", e)\n",
    "    \n",
    "#è¯ç¼–ç \n",
    "encoder[0].embed.load_state_dict(\n",
    "    params.text_model.embeddings.token_embedding.state_dict())\n",
    "\n",
    "#ä½ç½®ç¼–ç \n",
    "encoder[0].pos_embed.load_state_dict(\n",
    "    params.text_model.embeddings.position_embedding.state_dict())\n",
    "\n",
    "#12å±‚ç¼–ç å±‚\n",
    "for i in range(12):\n",
    "\n",
    "    #ç¬¬ä¸€å±‚norm\n",
    "    encoder[i + 1].s1[0].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].layer_norm1.state_dict())\n",
    "\n",
    "    #æ³¨æ„åŠ›qçŸ©é˜µ\n",
    "    encoder[i + 1].s1[1].q.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.q_proj.state_dict())\n",
    "\n",
    "    #æ³¨æ„åŠ›kçŸ©é˜µ\n",
    "    encoder[i + 1].s1[1].k.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.k_proj.state_dict())\n",
    "\n",
    "    #æ³¨æ„åŠ›vçŸ©é˜µ\n",
    "    encoder[i + 1].s1[1].v.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.v_proj.state_dict())\n",
    "\n",
    "    #æ³¨æ„åŠ›out\n",
    "    encoder[i + 1].s1[1].out.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].self_attn.out_proj.state_dict())\n",
    "\n",
    "    #ç¬¬äºŒå±‚norm\n",
    "    encoder[i + 1].s2[0].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].layer_norm2.state_dict())\n",
    "\n",
    "    #mlpç¬¬ä¸€å±‚fc\n",
    "    encoder[i + 1].s2[1].load_state_dict(\n",
    "        params.text_model.encoder.layers[i].mlp.fc1.state_dict())\n",
    "\n",
    "    #mlpç¬¬äºŒå±‚fc\n",
    "    encoder[i + 1].s3.load_state_dict(\n",
    "        params.text_model.encoder.layers[i].mlp.fc2.state_dict())\n",
    "\n",
    "#è¾“å‡ºnorm\n",
    "encoder[13].load_state_dict(params.text_model.final_layer_norm.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3488,  0.0139, -0.0409,  ..., -0.4707, -0.2910,  0.0627],\n",
      "         [ 0.6009, -0.4915,  1.0705,  ...,  0.0032,  0.5970, -0.4605],\n",
      "         [ 0.5848, -1.8402,  0.6390,  ...,  0.3736,  0.1611,  1.0529],\n",
      "         ...,\n",
      "         [ 0.7383, -0.1099,  1.2613,  ...,  0.2626, -0.2641,  0.3401],\n",
      "         [ 1.1845, -0.1865,  1.5217,  ...,  0.2758,  0.1133,  0.1809],\n",
      "         [ 0.9668, -0.5271,  1.4090,  ..., -0.0710,  0.1474, -0.2603]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "ğŸ‘€ ä¸¤ä¸ªæ¨¡å‹çš„å¼ é‡æ˜¯å¦ç›¸è¿‘ True\n",
      "tensor(1.8835e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—æ¨¡å‹çš„è¾“å‡ºç»“æœ(æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹è·Ÿé¢„è®­ç»ƒçš„æ¨¡å‹, ç†è®ºä¸Šè¾“å‡ºåº”è¯¥æ˜¯ä¸€æ ·çš„)\n",
    "a = encoder(torch.arange(77).unsqueeze(dim=0))\n",
    "b = params(torch.arange(77).unsqueeze(dim=0)).last_hidden_state\n",
    "\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "# (a==b).all() # æ‰“å°åº”è¯¥æ˜¯: True\n",
    "# æ¯”è¾ƒå®ƒä»¬æ˜¯å¦åœ¨å¯æ¥å—çš„è¯¯å·®èŒƒå›´å†…ç›¸ç­‰\n",
    "are_close = torch.allclose(a, b, atol=1e-4)\n",
    "\n",
    "print(\"ğŸ‘€ ä¸¤ä¸ªæ¨¡å‹çš„å¼ é‡æ˜¯å¦ç›¸è¿‘\", are_close)  # è¿™åº”è¯¥ä¼šè¾“å‡º True\n",
    "\n",
    "\n",
    "# æ‰“å°å·®å¼‚æœ€å¤§å€¼\n",
    "print((a - b).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
