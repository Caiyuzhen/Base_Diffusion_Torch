{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行环境\n",
    "`diffusers == 0.121`\n",
    "\n",
    "# VAE 的作用\n",
    "VAE 可以从潜在空间（latent space）中生成新的样本。通过学习数据的潜在表示，VAE 可以生成与训练数据相似的新数据。例如，可以用 VAE 生成新\n",
    "的图像、音频、文本等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 10, 10])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 定义残差连接层的类\n",
    "class Resnet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义一个序列化的神经网络模块\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32,  # 归一化层，分成32组\n",
    "                               num_channels=dim_in,  # 输入通道数\n",
    "                               eps=1e-6,  # 防止除零\n",
    "                               affine=True),  # 允许缩放和平移\n",
    "            torch.nn.SiLU(),  # SiLU激活函数\n",
    "            torch.nn.Conv2d(dim_in,  # 输入通道数\n",
    "                            dim_out,  # 输出通道数\n",
    "                            kernel_size=3,  # 卷积核大小\n",
    "                            stride=1,  # 步幅\n",
    "                            padding=1),  # 填充，使得输出大小与输入相同\n",
    "            torch.nn.GroupNorm(num_groups=32,  # 归一化层，分成32组\n",
    "                               num_channels=dim_out,  # 输出通道数\n",
    "                               eps=1e-6,  # 防止除零\n",
    "                               affine=True),  # 允许缩放和平移\n",
    "            torch.nn.SiLU(),  # SiLU激活函数\n",
    "            torch.nn.Conv2d(dim_out,  # 输入通道数\n",
    "                            dim_out,  # 输出通道数\n",
    "                            kernel_size=3,  # 卷积核大小\n",
    "                            stride=1,  # 步幅\n",
    "                            padding=1),  # 填充，使得输出大小与输入相同\n",
    "        )\n",
    "\n",
    "        # 如果输入和输出的通道数不一致，则使用1x1卷积调整输入的通道数\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in,  # 输入通道数\n",
    "                                       dim_out,  # 输出通道数\n",
    "                                       kernel_size=1,  # 1x1卷积核\n",
    "                                       stride=1,  # 步幅\n",
    "                                       padding=0)  # 无填充\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x的形状为 [batch_size, channels, height, width]，这里为 [1, 128, 10, 10]\n",
    "        res = x  # 保存输入，作为残差项\n",
    "        if self.res:\n",
    "            # 如果需要调整通道数，使用1x1卷积调整残差的形状为 [1, 256, 10, 10]\n",
    "            res = self.res(x)\n",
    "\n",
    "        # 计算主分支的输出，并与残差相加\n",
    "        # 主分支的输出形状为 [1, 256, 10, 10]\n",
    "        return res + self.s(x)  # 残差相加后的输出\n",
    "\n",
    "# 测试 Resnet 类，输入形状为 [1, 128, 10, 10]，期望输出形状为 [1, 256, 10, 10]\n",
    "Resnet(128, 256)(torch.randn(1, 128, 10, 10)).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VAE 自注意力层类(单头自注意力)\n",
    "class Atten(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # norm 层\n",
    "        self.norm = torch.nn.GroupNorm(num_channels=512,\n",
    "                                       num_groups=32,\n",
    "                                       eps=1e-6,\n",
    "                                       affine=True)\n",
    "\n",
    "        \"\"\"\n",
    "        在注意力机制中，我们对每个输入元素（例如句子中的一个单词）生成三个向量：\n",
    "            查询向量 (Query Vector, Q)\n",
    "            键向量 (Key Vector, K)\n",
    "            值向量 (Value Vector, V)\n",
    "        \"\"\"\n",
    "        self.q = torch.nn.Linear(512, 512)\n",
    "        self.k = torch.nn.Linear(512, 512)\n",
    "        self.v = torch.nn.Linear(512, 512)\n",
    "        self.out = torch.nn.Linear(512, 512)\n",
    "\n",
    "    # 单头, 无 mask\n",
    "    def forward(self, x):\n",
    "        #x -> [1, 512, 64, 64]\n",
    "        res = x\n",
    "\n",
    "        #norm,维度不变\n",
    "        #[1, 512, 64, 64]\n",
    "        x = self.norm(x)\n",
    "\n",
    "        #[1, 512, 64, 64] -> [1, 512, 4096] -> [1, 4096, 512]\n",
    "        x = x.flatten(start_dim=2).transpose(1, 2)\n",
    "\n",
    "        #线性运算,维度不变\n",
    "        #[1, 4096, 512]\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        #[1, 4096, 512] -> [1, 512, 4096]\n",
    "        k = k.transpose(1, 2)\n",
    "\n",
    "        #[1, 4096, 512] * [1, 512, 4096] -> [1, 4096, 4096]\n",
    "        #0.044194173824159216 = 1 / 512**0.5\n",
    "        \n",
    "\n",
    "        # 照理来说应该是等价的,但是却有很小的误差\n",
    "        # atten = q.bmm(k) * 0.044194173824159216\n",
    "        atten = torch.baddbmm(torch.empty(1, 4096, 4096, device=q.device),\n",
    "                              q,\n",
    "                              k,\n",
    "                              beta=0,\n",
    "                              alpha=0.044194173824159216)\n",
    "\n",
    "        atten = torch.softmax(atten, dim=2)\n",
    "\n",
    "        #[1, 4096, 4096] * [1, 4096, 512] -> [1, 4096, 512]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "        #线性运算,维度不变\n",
    "        #[1, 4096, 512]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        #[1, 4096, 512] -> [1, 512, 4096] -> [1, 512, 64, 64]\n",
    "        atten = atten.transpose(1, 2).reshape(-1, 512, 64, 64)\n",
    "\n",
    "        # 残差连接,维度不变\n",
    "        # [1, 512, 64, 64]\n",
    "        atten = atten + res\n",
    "\n",
    "        return atten\n",
    "\n",
    "\n",
    "Atten()(torch.randn(1, 512, 64, 64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 工具层\n",
    "class Pad(torch.nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 给数据的最右边跟最下边增加一行跟一列的 0\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1),\n",
    "                                       mode='constant',\n",
    "                                       value=0)\n",
    "\n",
    "\n",
    "Pad()(torch.ones(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VAE 模型类\n",
    "# 分为 encode 跟 decode 两个参数\n",
    "\n",
    "import torch\n",
    "\n",
    "# 定义变分自编码器（VAE）模型类，包含编码器和解码器\n",
    "class VAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义编码器部分\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            # 输入层：卷积操作，将输入图像的通道数从3变为128\n",
    "            torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            # 下采样和特征提取部分（多层 ResNet 模块和卷积下采样）\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),  # ResNet 残差模块，输入和输出通道均为128\n",
    "                Resnet(128, 128),  # 再次通过 ResNet 模块\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),  # 填充操作\n",
    "                    torch.nn.Conv2d(128, 128, 3, stride=2, padding=0),  # 卷积下采样，输出特征图尺寸减半\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 256),  # ResNet 模块，通道数从128增加到256\n",
    "                Resnet(256, 256),  # 再次通过 ResNet 模块\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(256, 256, 3, stride=2, padding=0),  # 再次下采样\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(256, 512),  # ResNet 模块，通道数从256增加到512\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(512, 512, 3, stride=2, padding=0),  # 再次下采样\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),  # 保持通道数为512的 ResNet 模块\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "            ),\n",
    "\n",
    "            # 中间层：增加注意力机制\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),  # ResNet 模块\n",
    "                Atten(),  # 注意力机制模块\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "            ),\n",
    "\n",
    "            # 输出层：标准化和卷积操作\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=512, num_groups=32, eps=1e-6),  # 组归一化\n",
    "                torch.nn.SiLU(),  # SiLU 激活函数\n",
    "                torch.nn.Conv2d(512, 8, 3, padding=1),  # 将特征图通道数降为8\n",
    "            ),\n",
    "\n",
    "            # 最后一层，用于生成正态分布的均值和方差\n",
    "            torch.nn.Conv2d(8, 8, 1),  # 1x1 卷积，保持通道数为8\n",
    "        )\n",
    "\n",
    "        # 定义解码器部分\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            # 输入层：从正态分布中采样得到的4个通道\n",
    "            torch.nn.Conv2d(4, 4, 1),  # 1x1 卷积，保持通道数为4\n",
    "\n",
    "            # 初始卷积操作，将通道数从4变为512\n",
    "            torch.nn.Conv2d(4, 512, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            # 中间部分：ResNet 模块和注意力机制\n",
    "            torch.nn.Sequential(Resnet(512, 512), Atten(), Resnet(512, 512)),\n",
    "\n",
    "            # 上采样部分（多层 ResNet 模块和上采样操作）\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),  # ResNet 模块，保持通道数为512\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),  # 上采样，特征图尺寸加倍\n",
    "                torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 卷积，保持通道数为512\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),  # ResNet 模块，保持通道数为512\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "                Resnet(512, 512),  # 再次通过 ResNet 模块\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),  # 上采样，特征图尺寸加倍\n",
    "                torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),  # 卷积，保持通道数为512\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 256),  # ResNet 模块，将通道数从512减为256\n",
    "                Resnet(256, 256),  # 再次通过 ResNet 模块\n",
    "                Resnet(256, 256),  # 再次通过 ResNet 模块\n",
    "                torch.nn.Upsample(scale_factor=2.0, mode='nearest'),  # 上采样，特征图尺寸加倍\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),  # 卷积，保持通道数为256\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(256, 128),  # ResNet 模块，将通道数从256减为128\n",
    "                Resnet(128, 128),  # 再次通过 ResNet 模块\n",
    "                Resnet(128, 128),  # 再次通过 ResNet 模块\n",
    "            ),\n",
    "\n",
    "            # 输出层：标准化和卷积操作\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=128, num_groups=32, eps=1e-6),  # 组归一化\n",
    "                torch.nn.SiLU(),  # SiLU 激活函数\n",
    "                torch.nn.Conv2d(128, 3, 3, padding=1),  # 将通道数降为3，即恢复为RGB图像\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # 采样函数，从正态分布中生成新的数据\n",
    "    def sample(self, h):\n",
    "        # h -> [1, 8, 64, 64]\n",
    "\n",
    "        # 将特征图的前4个通道作为均值，后4个通道作为方差\n",
    "        mean = h[:, :4]\n",
    "        logvar = h[:, 4:]\n",
    "        std = logvar.exp() ** 0.5  # 计算标准差\n",
    "\n",
    "        # 从正态分布中采样，并使用重参数技巧生成新的特征图\n",
    "        h = torch.randn(mean.shape, device=mean.device)\n",
    "        h = mean + std * h\n",
    "\n",
    "        # 返回符合正态分布的 h\n",
    "        return h\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 👇 1 表示一张图片\n",
    "        # x -> [1, 3, 512, 512]\n",
    "\n",
    "        # 编码：将输入图像编码为隐变量\n",
    "        h = self.encoder(x)  # h -> [1, 8, 64, 64]\n",
    "\n",
    "        # 从隐变量中采样\n",
    "        h = self.sample(h)  # h -> [1, 4, 64, 64]\n",
    "\n",
    "        # 解码：将隐变量解码为图像\n",
    "        h = self.decoder(h)  # h -> [1, 3, 512, 512]\n",
    "\n",
    "        return h  # 返回重建的图像\n",
    "\n",
    "# 测试 VAE 模型\n",
    "VAE()(torch.randn(1, 3, 512, 512)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "The config attributes {'scaling_factor': 0.18215} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "/Users/zeno/mambaforge/envs/torch_gpu_env/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_attention_op', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_use_memory_efficient_attention_xformers', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'channels', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'group_norm', 'half', 'ipu', 'key', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_head_size', 'num_heads', 'parameters', 'proj_attn', 'query', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'rescale_output_factor', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'set_use_memory_efficient_attention_xformers', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'value', 'xpu', 'zero_grad']\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_attention_op', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_use_memory_efficient_attention_xformers', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'channels', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'group_norm', 'half', 'ipu', 'key', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_head_size', 'num_heads', 'parameters', 'proj_attn', 'query', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'rescale_output_factor', 'reshape_batch_dim_to_heads', 'reshape_heads_to_batch_dim', 'set_extra_state', 'set_use_memory_efficient_attention_xformers', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'value', 'xpu', 'zero_grad']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载【预训练模型】来初始化参数\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "#加载预训练模型的参数\n",
    "params = AutoencoderKL.from_pretrained(\n",
    "    'model/diffsion_from_scratch.params', subfolder='vae')\n",
    "\n",
    "vae = VAE()\n",
    "\n",
    "# 加载参数用的函数\n",
    "def load_res(model, param):\n",
    "    model.s[0].load_state_dict(param.norm1.state_dict())\n",
    "    model.s[2].load_state_dict(param.conv1.state_dict())\n",
    "    model.s[3].load_state_dict(param.norm2.state_dict())\n",
    "    model.s[5].load_state_dict(param.conv2.state_dict())\n",
    "\n",
    "    if isinstance(model.res, torch.nn.Module):\n",
    "        model.res.load_state_dict(param.conv_shortcut.state_dict())\n",
    "\n",
    "# 加载参数用的函数\n",
    "def load_atten(model, param):\n",
    "    print(dir(param)) \n",
    "    model.norm.load_state_dict(param.group_norm.state_dict())\n",
    "    model.q.load_state_dict(param.query.state_dict())\n",
    "    model.k.load_state_dict(param.key.state_dict())\n",
    "    model.v.load_state_dict(param.value.state_dict())\n",
    "    model.out.load_state_dict(param.proj_attn.state_dict())\n",
    "\n",
    "\n",
    "#encoder.in\n",
    "vae.encoder[0].load_state_dict(params.encoder.conv_in.state_dict())\n",
    "\n",
    "#encoder.down\n",
    "for i in range(4):\n",
    "    load_res(vae.encoder[i + 1][0], params.encoder.down_blocks[i].resnets[0])\n",
    "    load_res(vae.encoder[i + 1][1], params.encoder.down_blocks[i].resnets[1])\n",
    "\n",
    "    if i != 3:\n",
    "        vae.encoder[i + 1][2][1].load_state_dict(\n",
    "            params.encoder.down_blocks[i].downsamplers[0].conv.state_dict())\n",
    "\n",
    "#encoder.mid\n",
    "load_res(vae.encoder[5][0], params.encoder.mid_block.resnets[0])\n",
    "load_res(vae.encoder[5][2], params.encoder.mid_block.resnets[1])\n",
    "load_atten(vae.encoder[5][1], params.encoder.mid_block.attentions[0])\n",
    "\n",
    "#encoder.out\n",
    "vae.encoder[6][0].load_state_dict(params.encoder.conv_norm_out.state_dict())\n",
    "vae.encoder[6][2].load_state_dict(params.encoder.conv_out.state_dict())\n",
    "\n",
    "#encoder.正态分布层\n",
    "vae.encoder[7].load_state_dict(params.quant_conv.state_dict())\n",
    "\n",
    "#decoder.正态分布层\n",
    "vae.decoder[0].load_state_dict(params.post_quant_conv.state_dict())\n",
    "\n",
    "#decoder.in\n",
    "vae.decoder[1].load_state_dict(params.decoder.conv_in.state_dict())\n",
    "\n",
    "#decoder.mid\n",
    "load_res(vae.decoder[2][0], params.decoder.mid_block.resnets[0])\n",
    "load_res(vae.decoder[2][2], params.decoder.mid_block.resnets[1])\n",
    "load_atten(vae.decoder[2][1], params.decoder.mid_block.attentions[0])\n",
    "\n",
    "#decoder.up\n",
    "for i in range(4):\n",
    "    load_res(vae.decoder[i + 3][0], params.decoder.up_blocks[i].resnets[0])\n",
    "    load_res(vae.decoder[i + 3][1], params.decoder.up_blocks[i].resnets[1])\n",
    "    load_res(vae.decoder[i + 3][2], params.decoder.up_blocks[i].resnets[2])\n",
    "\n",
    "    if i != 3:\n",
    "        vae.decoder[i + 3][4].load_state_dict(\n",
    "            params.decoder.up_blocks[i].upsamplers[0].conv.state_dict())\n",
    "\n",
    "#decoder.out\n",
    "vae.decoder[7][0].load_state_dict(params.decoder.conv_norm_out.state_dict())\n",
    "vae.decoder[7][2].load_state_dict(params.decoder.conv_out.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试 encode\n",
    "data = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "a = vae.encoder(data)\n",
    "b = params.encode(data).latent_dist.parameters\n",
    "\n",
    "(a == b).all() # tensor(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试 decode\n",
    "data = torch.randn(1, 4, 64, 64)\n",
    "\n",
    "a = vae.decoder(data)\n",
    "b = params.decode(data).sample\n",
    "\n",
    "(a == b).all() # tensor(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
